{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chris Clifford\n",
    "# CS 613 - Final Project\n",
    "# 12/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas\n",
    "import pytz\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://api.tdameritrade.com/v1'}\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "with open(r'config/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(config['ameritrade-api'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'd0d910b673284f7a8668ab9a3a12a664'}\n"
     ]
    }
   ],
   "source": [
    "# Secret config\n",
    "# Email crc339@drexel.edu for access\n",
    "with open(r'config/config.secret.yaml') as file:\n",
    "    secret = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(secret['news-api'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1047605, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FILE = 'data/analyst_ratings_processed.csv'\n",
    "data = pandas.read_csv(DATA_FILE).to_numpy()\n",
    "# Remove index column\n",
    "data = data[:,1:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392091, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove news headlines that appear before/after market hours (US) or near open/close\n",
    "# This helps us get cleaner data (we don't care about volatility spikes during open)\n",
    "def during_market_hours(row):\n",
    "    date_str = row[1]\n",
    "    utc_time = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z').astimezone(pytz.utc)\n",
    "    hr = utc_time.hour\n",
    "    return 14 <= hr <= 19\n",
    "\n",
    "mask = np.array([during_market_hours(d) for d in data])\n",
    "data = data[mask]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392091, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert datetime to ms since epoch for Ameritrade API\n",
    "def to_ms_since_epoch(date_str):\n",
    "    try:\n",
    "        EPOCH = datetime.utcfromtimestamp(0).replace(tzinfo=pytz.UTC)\n",
    "\n",
    "        utc_time = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "        return int((utc_time - EPOCH).total_seconds() * 1000)\n",
    "    except TypeError or ValueError:\n",
    "        print(date_str)\n",
    "        print(f'Failed to convert datetime \"{date_str}\" to MS since epoch. Continuing...')\n",
    "\n",
    "data = np.array([[row[0], to_ms_since_epoch(row[1]), row[2]] for row in data])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (start - duration, start + duration)\n",
    "def timeframe(start_ms, duration, duration_type):\n",
    "    \n",
    "    actual_duration_ms = 0\n",
    "    \n",
    "    if duration_type == 'ms':\n",
    "        actual_duration_ms = duration\n",
    "    elif duration_type == 'sec':\n",
    "        actual_duration_ms = duration * 1000\n",
    "    elif duration_type == 'min':\n",
    "        actual_duration_ms = duration * 1000 * 60\n",
    "    elif duration_type == 'hr':\n",
    "        actual_duration_ms = duration * 1000 * 60 * 60\n",
    "    elif duration_type == 'day':\n",
    "        actual_duration_ms = duration * 1000 * 60 * 60 * 24\n",
    "        \n",
    "    end_ms = start_ms + actual_duration_ms\n",
    "    \n",
    "    return start_ms - actual_duration_ms, start_ms + actual_duration_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1826, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets the volume data for the time period for a ticker\n",
    "# Returns [] if there is some kind of error\n",
    "def get_volume_data(start, end, ticker):\n",
    "    MIN_DATE = 1585229820000\n",
    "    \n",
    "    if start < MIN_DATE:\n",
    "        return []\n",
    "    URL = config['ameritrade-api']['url']\n",
    "    API_KEY = secret['ameritrade-api']['key']\n",
    "    ENDPOINT = f'/marketdata/{ticker}/pricehistory'\n",
    "    \n",
    "    url = (f'{URL}'\n",
    "           f'{ENDPOINT}?'\n",
    "           f'apikey={API_KEY}&'\n",
    "            'frequency=10&'\n",
    "           f'startDate={start}&'\n",
    "           f'endDate={end}')\n",
    "       \n",
    "    response = requests.get(url)\n",
    "    json = response.json()\n",
    "    \n",
    "    if 'error' in json:\n",
    "        # print(start, end, json['error'])\n",
    "        return []\n",
    "    \n",
    "    d = np.array([(d['datetime'], d['volume']) for d in json['candles'] if start <= d['datetime'] < end])\n",
    "    \n",
    "    try:\n",
    "        dates = d[:,[0]]\n",
    "    except IndexError:\n",
    "        # print('No data found for time frame.')\n",
    "        return []\n",
    "    \n",
    "    vols = d[:,[1]]\n",
    "    d_std = (vols - np.mean(vols)) / np.std(vols)\n",
    "    time.sleep(.5)\n",
    "    return np.concatenate((dates, d_std), axis=1)\n",
    "\n",
    "LIMIT = 50000\n",
    "TIME_SPAN = 1\n",
    "# Get the volume data for each headline\n",
    "vols = [(title, int(date), get_volume_data(timeframe(int(date), 1, 'hr')[0], timeframe(int(date), 1, 'hr')[1], ticker)) for title, date, ticker in zip(data[:LIMIT, 0], data[:LIMIT, 1], data[:LIMIT, 2])]\n",
    "# Toss out bad data\n",
    "volume_data = np.array([row for row in vols if len(row[2]) > 0])\n",
    "volume_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the small, medium, and large volume spikes within the duration\n",
    "def vol_spikes(vold, start, duration, duration_type):\n",
    "    median = np.median(vold[:,1])\n",
    "    stddev = np.std(vold[:,1])\n",
    "\n",
    "    # print('Median: {:.3f}'.format(median))\n",
    "    # print('Std Dev: {:.3f}'.format(stddev))\n",
    "\n",
    "    end = timeframe(start, duration, duration_type)[1]\n",
    "\n",
    "    tf_vold = vold[(start <= vold[:,0]) & (vold[:,0] <= end)]\n",
    "    volumes = tf_vold[:,1]\n",
    "    if len(volumes) == 0:\n",
    "        return np.array([None, None, None])\n",
    "    initial = volumes[0]\n",
    "\n",
    "    upper = initial + stddev\n",
    "    lower = initial - stddev\n",
    "    spike65 = (volumes > upper).any() or (volumes < lower).any()\n",
    "    #print('65%: {:.3f} - {:.3f}'.format(lower, upper))\n",
    "    #print(f'Spike? {\"Yes\" if spike65 else \"No\"}')\n",
    "\n",
    "    upper = initial + 2*stddev\n",
    "    lower = initial - 2*stddev\n",
    "    spike95 = (volumes > upper).any() or (volumes < lower).any()\n",
    "    #print('95%: {:.3f} - {:.3f}'.format(lower, upper))\n",
    "    #print(f'Spike? {\"Yes\" if spike95 else \"No\"}')\n",
    "\n",
    "    upper = initial + 3*stddev\n",
    "    lower = initial - 3*stddev\n",
    "    spike99 = (volumes > upper).any() or (volumes < lower).any()\n",
    "    #print('99%: {:.3f} - {:.3f}'.format(lower, upper))\n",
    "    #print(f'Spike? {\"Yes\" if spike99 else \"No\"}')\n",
    "    #print('')\n",
    "    \n",
    "    return np.array([1 if spike65 else 0, 1 if spike95 else 0, 1 if spike99 else 0], dtype=None)\n",
    "\n",
    "temp = np.vstack([np.append(row[0], vol_spikes(row[2], row[1], 15, 'min')) for row in volume_data])\n",
    "out_data = temp[temp[:,1] != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the small, medium, and large volume spikes out to data files\n",
    "out_data_with_headers = np.concatenate((np.array([['title', '65%', '95%', '99%']]), out_data))\n",
    "\n",
    "out65 = out_data_with_headers[:,[0,1]]\n",
    "\n",
    "OUT_FILE = 'data/processed65.csv'\n",
    "with open(OUT_FILE, 'w', encoding='utf-8') as file:\n",
    "    csv.writer(file).writerows(out65)\n",
    "\n",
    "OUT_FILE = 'data/processed95.csv'\n",
    "out95 = out_data_with_headers[:,[0,2]]\n",
    "with open(OUT_FILE, 'w', encoding='utf-8') as file:\n",
    "    csv.writer(file).writerows(out95)\n",
    "\n",
    "OUT_FILE = 'data/processed99.csv'\n",
    "out99 = out_data_with_headers[:,[0,3]]\n",
    "with open(OUT_FILE, 'w', encoding='utf-8') as file:\n",
    "    csv.writer(file).writerows(out99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
